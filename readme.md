# ğŸ—¡ï¸ Slay the Spire Reinforcement Learning (RL) Agent

åŸºäº **Gymnasium** å’Œ **Stable-Baselines3 (Maskable PPO)** æ„å»ºçš„ã€Šæ€æˆ®å°–å¡”ã€‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒç¯å¢ƒã€‚

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªä¸æ¸¸æˆ **[CommunicationMod](https://github.com/ForgottenArbiter/CommunicationMod)** äº¤äº’çš„ Python ç¯å¢ƒï¼Œåœ¨è¿™ä¸ªç¯å¢ƒä¸­å®ç°å¯¹AIçš„è®­ç»ƒã€‚

## ğŸ› ï¸ ç›®å½•ç»“æ„

```text
Project Root/
â”œâ”€â”€ main.py                 # è®­ç»ƒå…¥å£ï¼Œé…ç½®æ¨¡å‹ä¸å›è°ƒå‡½æ•°
â”œâ”€â”€ spire_env/              # Gym ç¯å¢ƒåŒ…
â”‚   â”œâ”€â”€ env.py              # ä¸»ç¯å¢ƒç±» (SlayTheSpireEnv)ï¼Œè´Ÿè´£ç»„è£…å„æ¨¡å—
â”‚   â”œâ”€â”€ definitions.py      # åŠ¨ä½œç©ºé—´ä¸è§‚å¯Ÿç©ºé—´å®šä¹‰
â”‚   â”œâ”€â”€ interface.py        # åº•å±‚ Stdout/Stdin é€šè®¯æ¥å£
â”‚   â””â”€â”€ logic/              # [æ ¸å¿ƒ] é€»è¾‘å¤„ç†æ¨¡å—
â”‚       â”œâ”€â”€ game_io.py      # çŠ¶æ€è¯»å–ä¸ç¼“å†²åŒºæ¸…æ´—
â”‚       â”œâ”€â”€ combat.py       # æˆ˜æ–—åŒæ­¥ä¸é˜²æŠ–é€»è¾‘
â”‚       â”œâ”€â”€ navigator.py    # éæˆ˜æ–—åœºæ™¯è‡ªåŠ¨å¯¼èˆª (åœ°å›¾/äº‹ä»¶/å•†åº—)
â”‚       â””â”€â”€ reward.py       # å¥–åŠ±å‡½æ•°è®¡ç®—
â””â”€â”€ utils/
    â”œâ”€â”€ action_mapper.py    # åŠ¨ä½œç¼–è§£ç ä¸ Mask ç”Ÿæˆ
    â””â”€â”€ state_encoder.py    # çŠ¶æ€ç‰¹å¾æå– (State -> Vector)
```

## ğŸš€ ç¯å¢ƒæ­å»ºä¸è¿è¡Œ

### 1. æ¸¸æˆç«¯å‡†å¤‡
1.  å®‰è£… Steam ç‰ˆ **Slay the Spire**ã€‚
2.  è®¢é˜…å¹¶å¯ç”¨ä»¥ä¸‹åˆ›æ„å·¥åŠ Modï¼š
    * **ModTheSpire** (åŠ è½½å™¨)
    * **BaseMod** (åŸºç¡€åº“)
    * **[CommunicationMod](https://github.com/ForgottenArbiter/CommunicationMod)** (é€šè®¯æ¥å£)
    * *(å¯é€‰) SuperFastMode (æé€Ÿæ¨¡å¼ï¼Œæ¨èå¼€å¯ä»¥åŠ å¿«è®­ç»ƒ)*

### 2. é…ç½® CommunicationMod
ä½ éœ€è¦ä¿®æ”¹é…ç½®ï¼Œè®©æ¸¸æˆçŸ¥é“å¦‚ä½•å¯åŠ¨ Python è„šæœ¬ã€‚

1.  æ‰¾åˆ°é…ç½®æ–‡ä»¶ `communication_mod.config.properties`ã€‚
    * é»˜è®¤ä½ç½®é€šå¸¸åœ¨æ¸¸æˆå®‰è£…ç›®å½•ä¸‹çš„ `preferences` æ–‡ä»¶å¤¹å†…ã€‚
    * å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯·å‚è€ƒ Wiki è·å–ä¸åŒç³»ç»Ÿçš„å…·ä½“è·¯å¾„ï¼š[ModTheSpire Wiki - SpireConfig](https://github.com/kiooeht/ModTheSpire/wiki/SpireConfig)
2.  ä½¿ç”¨è®°äº‹æœ¬æ‰“å¼€ï¼Œä¿®æ”¹ `command` å­—æ®µæŒ‡å‘ä½ çš„ Python è§£é‡Šå™¨å’Œé¡¹ç›®è·¯å¾„ï¼š

```properties
# Windows ç¤ºä¾‹ (æ³¨æ„è·¯å¾„åˆ†éš”ç¬¦)
command=D:/Anaconda/envs/spire_ai/python.exe D:/Projects/SlayTheSpireAI/main.py
```

### 3. Python ç¯å¢ƒå‡†å¤‡
å»ºè®®ä½¿ç”¨ `venv` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶é€šè¿‡ `requirements.txt` å®‰è£…ä¾èµ–ï¼š

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
.\venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 3. å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt # ä½¿ç”¨æ¸…åæº
```

### 4. å¼€å§‹è®­ç»ƒ
1.  å¯åŠ¨ **Slay the Spire**ã€‚
2.  åœ¨ Mod å¯åŠ¨å™¨ä¸­å‹¾é€‰ä¸Šè¿° Modï¼Œç‚¹å‡» **Play**ã€‚
3.  æ¸¸æˆå¯åŠ¨åï¼ŒCommunicationMod ä¼šè‡ªåŠ¨æ‹‰èµ· Python è„šæœ¬ã€‚
4.  è§‚å¯Ÿ Python æ§åˆ¶å°æˆ– `logs/ai_debug_log.txt`ï¼Œçœ‹åˆ° `>>> ç¯å¢ƒé‡ç½® >>>` å³ä»£è¡¨è®­ç»ƒå¼€å§‹ã€‚

> **ğŸ’¡ æ€§èƒ½æç¤º**ï¼šè®­ç»ƒå¼€å§‹åï¼Œå»ºè®®å°†æ¸¸æˆçª—å£**æœ€å°åŒ–**ã€‚è¿™å¯ä»¥åœæ­¢æ¸¸æˆçš„å›¾å½¢æ¸²æŸ“ï¼Œæ˜¾è‘—é™ä½ CPU å ç”¨ï¼Œä»è€Œæå‡è®­ç»ƒ FPSã€‚

## ğŸ“Š è®­ç»ƒç›‘æ§ (Visualization)

æœ¬é¡¹ç›®é›†æˆäº† TensorBoard è®°å½•è®­ç»ƒæ›²çº¿ï¼ˆå¥–åŠ±å˜åŒ–ã€Loss ç­‰ï¼‰ã€‚
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç›‘æ§é¢æ¿ï¼š

```bash
tensorboard --logdir logs/sb3
```

å¯åŠ¨åï¼Œåœ¨æµè§ˆå™¨è®¿é—® http://localhost:6006 å³å¯æŸ¥çœ‹å®æ—¶å›¾è¡¨ã€‚

## ğŸ“ˆ å¥–åŠ±è®¾è®¡ (Reward Shaping)

ç›®å‰çš„å¥–åŠ±å‡½æ•° (`logic/reward.py`) åŒ…å«ï¼š
* **ä¼¤å®³å¥–åŠ±**: å¯¹æ•Œäººé€ æˆä¼¤å®³ (+)ã€‚
* **æ ¼æŒ¡å¥–åŠ±**: è·å¾—æœ‰æ•ˆæ ¼æŒ¡ï¼ˆä¸è¶…è¿‡æ•Œäººæ”»å‡»åŠ›ï¼‰(+)ã€‚
* **å‡»æ€å¥–åŠ±**: æ¶ˆç­æ•Œäºº (++)ã€‚
* **è¿‡å±‚å¥–åŠ±**: çˆ¬åˆ°ä¸‹ä¸€å±‚ (+)ã€‚
* **å—ä¼¤æƒ©ç½š**: è‡ªèº«æ‰è¡€ (-)ã€‚

## ğŸ“ TODO / æœªæ¥è®¡åˆ’
* [ ] å®Œå–„å•†åº—è´­ä¹°é€»è¾‘ï¼ˆç›®å‰æ˜¯å¼ºåˆ¶è·³è¿‡ï¼‰ã€‚
* [ ] å¢åŠ æ›´å¤æ‚çš„äº‹ä»¶å†³ç­–é€»è¾‘ï¼ˆç›®å‰å¤šä¸ºéšæœºæˆ–å›ºå®šï¼‰ã€‚
* [ ] æ¥å…¥ LSTM/Transformer ä»¥å¤„ç†é•¿çŸ­æœŸè®°å¿†ã€‚
* [ ] é€‚é…å…¶ä»–è§’è‰²ï¼ˆç›®å‰ä¸»è¦é€‚é…é“ç”²æˆ˜å£«ï¼‰ã€‚

---

**License**: MIT


## ğŸ§  Core Architecture: The "Brain" Upgrade

This project has evolved from a naive bot to a strategic agent through three fundamental architectural shifts: **Perception, Precision, and Values**.

### 1. Perception: From Hash to One-Hot (State Encoder)
* **The Problem**: Previously, cards were encoded using `zlib.crc32` hashing. To the Neural Network, "Strike" (Hash: 0.12) and "Perfected Strike" (Hash: -0.98) looked like completely unrelated random numbers. The AI was effectively "face-blind" to card identities.
* **The Solution**: We implemented a **One-Hot Encoding** system with a fixed vocabulary.
    * Each card now has a dedicated input dimension (neuron).
    * **Energy** is encoded as a One-Hot vector (0-5+) rather than a scalar, allowing the network to learn non-linear thresholds (e.g., "I can play this heavy card ONLY when energy is at state 4").
    * **Gold** is scaled using `Log10` to make the AI sensitive to early-game economy differences (50 vs 150 gold) while ignoring late-game inflation.

### 2. Decision: Action Space Flattening (Action Mapper)
* **The Problem**: The original action space was `Discrete(14)` (Card 1-10, Potion 1-3). The AI could decide *which* card to play, but not *who* to target. It defaulted to attacking the first monster, making it impossible to prioritize high-threat enemies (e.g., killing the Snecko first).
* **The Solution**: We flattened the action space to `Discrete(67)`.
    * **Formula**: `ActionID = (CardIndex * 5_Targets) + TargetIndex`.
    * This gives the AI a "sniper scope," allowing it to output specific commands like "Play Bash on Monster #2."
    * A smart `ActionMapper` dynamically masks invalid targets (dead monsters) to prune the search space.

### 3. Values: The "Fear Factor" (Reward Shaping)
* **The Problem**: A standard linear reward function treats all HP loss equally. Losing 5 HP when at 80/80 health is a minor inconvenience; losing 5 HP at 6/80 health is fatal. A linear agent often dies because it greedily trades health for damage.
* **The Solution**: We introduced a **Non-Linear Survival Penalty**.
    * **The Formula**: 
      $$R_{loss} = \text{BaseLoss} \times (1 + 2 \times (1 - \text{HPRatio})^2)$$
    * **Behavior**:
        * At **100% HP**: The penalty multiplier is ~1.0x. (Aggressive)
        * At **10% HP**: The penalty multiplier spikes to ~2.6x. (Defensive)
    * **Resource Management**: Using a potion now incurs a small negative reward (-3.0). This teaches the AI a concept of "Cost," encouraging it to save potions for Elite/Boss fights rather than wasting them on weak minions.

## ğŸ”„ æ›´æ–°æ—¥å¿— (Changelog)
### v1.0.4 - Action Space Upgrade 
* **Targeted Capability**: The agent is no longer limited to attacking the first monster. It can now choose specific targets for cards and potions.
    * *Mechanism*: Action space flattened to `Discrete(67)`.
    * *Logic*: `ActionID = (CardIndex * 5) + TargetIndex`.
* **Smart Masking**: The `ActionMapper` now intelligently masks invalid targets (e.g., dead monsters) for targeted cards, while automatically defaulting to "Target 0" for AOE/Power cards to reduce search space.

### v1.0.3 - Perception Refactor 

#### ğŸ§  æ¨¡å‹æ¶æ„å˜æ›´ (AI Model Architecture)
* **çŠ¶æ€ç¼–ç é‡æ„ (State Encoder Overhaul)**:
    * **From Hash to One-Hot**: ä»¥å‰ä½¿ç”¨éšæœºå“ˆå¸Œå€¼ä»£è¡¨å¡ç‰Œï¼ˆå¦‚ `Strike = 0.123`ï¼‰ï¼Œå¯¼è‡´ç¥ç»ç½‘ç»œéš¾ä»¥æ”¶æ•›ã€‚ç°åœ¨æ„å»ºäº†å›ºå®šè¯è¡¨ï¼Œä½¿ç”¨ **One-Hot Encoding** ç‹¬ç«‹ç»´åº¦è¡¨ç¤ºæ¯å¼ å¡ç‰Œã€‚
    * *Effect*: AI ç°åœ¨èƒ½åƒäººç±»ä¸€æ ·å‡†ç¡®åŒºåˆ†â€œæ‰“å‡»â€å’Œâ€œé˜²å¾¡â€ï¼Œè€Œä¸æ˜¯å¤„ç†æ¨¡ç³Šçš„æµ®ç‚¹æ•°ã€‚
* **ç‰¹å¾å·¥ç¨‹ä¼˜åŒ– (Feature Engineering)**:
    * **Gold (é‡‘å¸)**: çº¿æ€§å½’ä¸€åŒ– $\rightarrow$ **å¯¹æ•°ç¼©æ”¾ (`log10`)**ã€‚æé«˜äº†æ¨¡å‹å¯¹ä½é‡‘å¸æ•°é‡å˜åŠ¨çš„æ•æ„Ÿåº¦ã€‚
    * **Energy (èƒ½é‡)**: æ ‡é‡æ•°å€¼ $\rightarrow$ **One-Hot å‘é‡**ã€‚å¸®åŠ©æ¨¡å‹ç†è§£èƒ½é‡çš„éçº¿æ€§é˜ˆå€¼ï¼ˆå¦‚ 3è´¹å’Œ 4è´¹çš„è´¨å˜ï¼‰ã€‚
    * **HP (è¡€é‡)**: æ–°å¢ **"æ¿’æ­»çŠ¶æ€" (Critical Health)** å¸ƒå°”ç‰¹å¾ï¼ˆHP < 15%ï¼‰ï¼Œå¼ºåŒ–ç”Ÿå­˜æœ¬èƒ½ã€‚